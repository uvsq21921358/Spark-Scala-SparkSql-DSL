{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Prenom(sexe: String, prenom: String, annee: Int, codeDept: Int, nombre: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val prenomsRDD = sc.textFile(\"prenoms.txt\").filter(l => l.startsWith(\"sexe\") == false).filter(l => l.contains(\"XX\") == false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Pour les conversions implicites de RDDs vers DataFrames\n",
    "val sparkRO = spark // bricolage pour que cela fonctionne dans le notebool (inutile sinon)\n",
    "import sparkRO.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[sexe: string, prenom: string ... 3 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prenoms = prenomsRDD.map(_.split('\\t')).map(a => Prenom(a(0), a(1), a(2).toInt, a(3).toInt, a(4).toDouble.toInt)).toDS()\n",
    "prenoms.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde dans les différents formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file._\n",
    "import java.nio.file.attribute.BasicFileAttributes\n",
    "import java.util.concurrent.atomic.AtomicLong\n",
    "\n",
    "def pathSize(path: Path): Long = {\n",
    "    var size = new AtomicLong(0)\n",
    "\n",
    "    Files.walkFileTree(path, new SimpleFileVisitor[Path]() {\n",
    "        override def visitFile(file: Path, attrs: BasicFileAttributes): FileVisitResult = {\n",
    "            size.addAndGet(attrs.size())\n",
    "            FileVisitResult.CONTINUE\n",
    "        }\n",
    "    });\n",
    "    size.get()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "val formats = Map(\n",
    "    \"csv\" -> List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\"),\n",
    "    \"json\" -> List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\"),\n",
    "    \"parquet\" -> List(\"uncompressed\", \"gzip\", \"snappy\"),\n",
    "    \"orc\" -> List(\"uncompressed\", \"snappy\", \"zlib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde au format csv compressé en uncompressed : "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n",
       "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n",
       "  at $anonfun$1$$anonfun$apply$1.apply(<console>:54)\n",
       "  at $anonfun$1$$anonfun$apply$1.apply(<console>:52)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at $anonfun$1.apply(<console>:52)\n",
       "  at $anonfun$1.apply(<console>:51)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.MapLike$DefaultKeySet.foreach(MapLike.scala:174)\n",
       "  ... 48 elided\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 11, localhost, executor driver): java.lang.ClassCastException: Prenom cannot be cast to Prenom\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
       "\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n",
       "  ... 89 more\n",
       "Caused by: java.lang.ClassCastException: Prenom cannot be cast to Prenom\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
       "  at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
       "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (format <- formats.keys) {\n",
    "    for (codec <- formats(format)) {\n",
    "        prenoms.write.mode(\"overwrite\").option(\"compression\", codec).format(format).save(\"prenoms\")\n",
    "        val prenomsPath = Paths.get(\"prenoms\")\n",
    "        println(s\"$format, $codec, \" + pathSize(prenomsPath))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultats = \"\"\"csv, uncompressed, 68804952                                                     \n",
    "csv, bzip2, 10676889                                                            \n",
    "csv, deflate, 11998839          \n",
    "csv, gzip, 11998875                                                             \n",
    "json, uncompressed, 238735998                                                   \n",
    "json, bzip2, 9491007                                                            \n",
    "json, deflate, 14662059                                                         \n",
    "json, gzip, 14662095                                                            \n",
    "parquet, uncompressed, 8344008                                                  \n",
    "parquet, gzip, 4820150                                                          \n",
    "parquet, snappy, 6476035                                 \n",
    "orc, uncompressed, 13265926                                                     \n",
    "orc, snappy, 5810818                                                            \n",
    "orc, zlib, 4377612\"\"\"\n",
    "\n",
    "val sizes = resultats.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map((orc,snappy) -> 5810818, (json,deflate) -> 14662059, (json,bzip2) -> 9491007, (csv,uncompressed) -> 68804952, (parquet,snappy) -> 6476035, (csv,deflate) -> 11998839, (csv,gzip) -> 11998875, (json,uncompressed) -> 238735998, (parquet,gzip) -> 4820150, (orc,uncompressed) -> 13265926, (csv,bzip2) -> 10676889, (json,gzip) -> 14662095, (parquet,uncompressed) -> 8344008, (orc,zlib) -> 4377612)\n"
     ]
    }
   ],
   "source": [
    "var data = scala.collection.mutable.Map[Tuple2[String, String], Long]()\n",
    "for (line <- sizes) {\n",
    "    val cols = line.split(\",\").map(_.trim)\n",
    "    data += ((cols(0), cols(1)) -> cols(2).toLong)\n",
    "}\n",
    "println(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taille (en Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format | uncomp.  | bzip2    | def.     | gzip     | snappy   | zlib     |\n",
      "csv    |    65.62 |    10.18 |    11.44 |    11.44 |     0.00 |     0.00 |\n",
      "json   |   227.68 |     9.05 |    13.98 |    13.98 |     0.00 |     0.00 |\n",
      "parquet|     7.96 |     0.00 |     0.00 |     4.60 |     6.18 |     0.00 |\n",
      "orc    |    12.65 |     0.00 |     0.00 |     0.00 |     5.54 |     4.17 |\n"
     ]
    }
   ],
   "source": [
    "val fileFormats = List(\"csv\", \"json\", \"parquet\", \"orc\")\n",
    "val codecs = List(\"uncompressed\", \"bzip2\", \"deflate\", \"gzip\", \"snappy\", \"zlib\")\n",
    "println(\"Format | uncomp.  | bzip2    | def.     | gzip     | snappy   | zlib     |\")\n",
    "for (format <- fileFormats) {\n",
    "    print(f\"$format%-7s|\")\n",
    "    for (codec <- codecs) {\n",
    "        val size = data.getOrElse[Long]((format, codec), 0) / (1024.0 * 1024)\n",
    "        print(f\"$size%9.2f |\")\n",
    "    }\n",
    "    println\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
